<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Buscar"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-决策树与随机森林" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/12/09/决策树与随机森林/" class="article-date">
  <time datetime="2017-12-09T14:03:57.000Z" itemprop="datePublished">2017-12-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/09/决策树与随机森林/">决策树与随机森林</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>##1. 信息熵</p>
<ul>
<li>熵：用以描述信息量的大小，当一个小概率事件发生时，它所蕴含的信息就是大的。公式：-$\sum_{i=1}^n$$p_i*log(p_i)$。</li>
<li>相对熵：又称互熵，设p(x)、q(x)是X中取值的两个概率分布，则p对q的相对熵是$$D(p||q) = \sum<em>x p(x)log{p(x) \over q(x)} = E</em>{p(x)}log{p(x) \over q(x)}$$；用于度量两个随机变量的距离.</li>
<li>联合熵：两个随机变量所包含的信息量</li>
<li>条件熵：$H(Y|X)=H(X,Y)-H(X)$,含义：(X,Y)发生所包含的熵，减去X单独发生时包含的熵，也可以表达为：在X发生的前提下，Y发生“新”带来的熵。定义式：$H(X,Y)-H(X)=-\sum<em>{x,y}p(x,y)logp(y|x)$ = $\sum</em>{x}p(x)H(Y|X=x)$（加权平均）</li>
<li>互信息：随机变量X与Y相互重叠的信息，形象化表示为维恩图中的X,Y交集部分。定义为X,Y的联合分布和独立分布乘积的相对熵。即：$$I(X,Y)=D(P(X,Y)||P(X)P(Y))$$</li>
<li>经验熵和经验条件熵：当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵。</li>
</ul>
<p>##2. 决策树（Decision Tree）</p>
<p>###2.1 性质</p>
<ol>
<li>树形结构，叶节点表示一种类别</li>
<li>以实例为基础的归纳学习</li>
<li>基本思想是以信息熵为度量构造熵值下降最快的树</li>
</ol>
<p>###2.2 特点</p>
<ol>
<li>监督学习，自学习</li>
<li>从一类无序、无规则的事务中推理出分类规则</li>
</ol>
<p>###2.3 生成算法</p>
<ul>
<li>ID3:Iterative Dichotomiser</li>
<li>C4.5</li>
<li>CART：Classification And Regression Tree</li>
</ul>
<p>###2.4 学习算法</p>
<p>####2.4.1 信息增益</p>
<ol>
<li>信息增益：$g(D|A)=H(D)-H(D|A)$–表示得知特征A的信息而使得X的信息的不确定减少的程度。从另一个角度来说：训练数据集与特征A的互信息</li>
<li>计算方法：<br>(1）先计算数据集D的经验熵$H(D)=-\sum<em>{k=1}^K\frac{|C</em>{k}|}{|D|}log\frac{|C_{k}|}{|D|}$<br>(2) 遍历所有特征，对于任一个特征A：先计算经验条件熵:$H(D|A)$，然后计算信息增益：$g(D,A)=H(D)-H(D|A)$，最后选择信息增益最大的特征作为当前分裂的特征</li>
</ol>
<p>####2.4.2 信息增益率</p>
<ol>
<li>信息增益率：$g_{r}(D,A)=g(D|A)/H(A)$</li>
<li>信息增益会导致特征数量越多的特征成为信息增益的首选特征，从而导致一些没有意义的特征被选取成增益特征，例如：编号。增益率解决了这个问题。</li>
</ol>
<p>####2.4.3 Gini系数</p>
<ol>
<li>Gini：$Gini(p)=\sum<em>{k=1}^{K}p</em>{k}(1-p<em>{k})=1-\sum</em>{k=1}^{K}p<em>{k}^{2}=1-\sum</em>{k=1}^{K}(\frac{|C_k|}{|D|})^2$</li>
<li>讨论：$H(X)=-\sum_{k=1}^{K}p_klnp<em>k\approx\sum</em>{k=1}^{K}p<em>{k}(1-p</em>{k})$</li>
<li>Gini系数的第二定义：经济邻域，表示贫富差距的衡量</li>
</ol>
<p>####2.4.4 学习算法总结</p>
<ol>
<li>ID3：使用信息增益/互信息进行特征选择</li>
</ol>
<blockquote>
<p>取值多的属性，更容易使得数据更纯，其信息增益更大，极端例子：编号<br>最终得到的是一棵庞大但是深度浅的树，不合理</p>
</blockquote>
<ol>
<li>C4.5：信息增益率作为学习算法</li>
<li>CART:基尼系数</li>
</ol>
<p>###2.5 决策树的评价</p>
<ol>
<li>损失函数：$C(T)=\sum<em>{t\in leaf}N</em>{t}H(t)$,对所有的叶节点的熵进行求和，该值越小越能说明对样本的分类越精确。</li>
<li><strong>各叶结点包含的样本数目不同，可使用样本数加权求熵和</strong></li>
</ol>
<p>###2.6 过拟合处理方法</p>
<p>####2.6.1 剪枝</p>
<blockquote>
<p>剪枝总体思路：</p>
<ol>
<li>由完全树$T_0$开始，剪枝部分节点得到$T$,再次剪枝部分节点得到$T$，直到仅剩根的树</li>
<li>在验证数据集上对这K个树分别评价，选择损失函数最小的树$T_\alpha$</li>
</ol>
</blockquote>
<ol>
<li>预剪枝：规定树的深度；设定熵的最小阈值；规定叶子节点的最少数量</li>
<li>后剪枝：选择最小的$\alpha$，对子树进行合并，迭代进行从而得到一系列的$T_0\;T_1\;T_2\cdot\cdot\cdot T_N$</li>
</ol>
<p>#####2.6.1.1 后剪枝</p>
<ol>
<li>修正损失函数：$C(T)=\sum_{t\in leaf}N_t\cdot H(t)$</li>
<li>假定当前对$\gamma$为根的子树剪枝，剪枝后只保留节点本身而删除子树的所有节点</li>
<li>计算以$\gamma$为为根的子树，分别计算剪枝前后的损失函数，即$C<em>{\alpha}(r)=C(r)+\alpha$和$C</em>{\alpha}(R)=C(r)+\alpha\cdot|R<em>{leaf}|$；使两个等式相等求得$\alpha=\frac{C(r)-C(R)}{\langle R</em>{leaf}-1 \rangle}$。$\alpha$称为剪枝系数</li>
</ol>
<p>#####2.6.1.2 剪枝算法</p>
<ol>
<li>计算所有内部节点的剪枝系数</li>
<li>查找最小的剪枝系数的节点，剪枝得到决策树$T_k$；</li>
<li>重复以上步骤，直到得到只有一个节点的决策树</li>
<li>得到决策树序列$T_0\;T_1\;T_2\cdot\cdot\cdot T_N$</li>
<li>使用验证集选择最优子树</li>
</ol>
<hr>
<p>##3 随机森林</p>
<p>###3.1 Bagging</p>
<ol>
<li>从样本集有放回重复采样n个，m次</li>
<li>使用这n个样本建立m个分类器</li>
<li>输入数据，根据m个投票结果决定数据是哪一类型</li>
</ol>
<p>###3.2 随机森林 – 随机样本，随机属性</p>
<ol>
<li>首先进行Bootstrap采样n个样本，总共进行m次</li>
<li>然后从所有属性里头选取k个属性，分别建立m棵决策树</li>
<li>最后，投票表决</li>
</ol>
<p>###3.3 随机森林/Bagging和决策树的关系</p>
<ol>
<li>随机森林使用决策树作为基本分类器</li>
<li>也可以使用其他弱分类器组合成随机森林</li>
</ol>
<p>##4 样本不均衡的常用处理方法<br><strong>假定A类数量大于B类，且严重不均衡</strong></p>
<p>###4.1 A类欠采样Undersampling</p>
<ul>
<li>随机欠采样</li>
<li>A类分成若干子类</li>
<li>采用聚类对A类进行分割</li>
</ul>
<p>###4.2 B类过采样Oversampling</p>
<blockquote>
<p>避免欠采样造成的信息丢失</p>
</blockquote>
<p>###4.3 B类数据合成Synthetic Data Generation</p>
<ul>
<li>随机插值得到新样本</li>
<li>SMOTE</li>
</ul>
<p>###4.4 代价敏感学习 </p>
<blockquote>
<p>降低A类权值，提高B类权值</p>
</blockquote>
<p>##5. 总结</p>
<ol>
<li>决策树/随机森林逻辑简单，符合人类做判断的逻辑</li>
<li>随机森林的集大智（aggregation）的思想，可以用在其他分类器的设计中</li>
<li>数据的处理过程中需要考虑<strong>数据平衡</strong>问题</li>
</ol>
<p>##6. 思考题</p>
<ol>
<li>思考随机森林为何可以提高正确率，且降低过拟合程度？</li>
</ol>
<blockquote>
<p>答：从概率学角度来说，多个弱分类器进行叠加使用会使得分类正确的可能性大大提升。随机森林综合考虑多个决策树，所以从一定程度上可以降低拟合程度</p>
</blockquote>
<ol>
<li>决策树后剪枝可以怎么操作？</li>
</ol>
<blockquote>
<p>答：计算决策树内部节点的剪枝系数，从小到大依次进行剪枝得到对应k棵树，最后使用验证集选取经验熵最小的那棵树木</p>
</blockquote>
<ol>
<li>请解释系数为何可以用于分类标准。</li>
</ol>
<blockquote>
<p>Gini系数从公式上类似于熵，可以看成是熵的系数在x=1处的一阶泰勒展开</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/12/09/决策树与随机森林/" data-id="cjazf6zvo0000gfs6halxm7kx" class="article-share-link">Compartir</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/决策树、随机森林、机器学习/">决策树、随机森林、机器学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-My-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/12/09/My-blog/" class="article-date">
  <time datetime="2017-12-09T14:00:19.000Z" itemprop="datePublished">2017-12-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/09/My-blog/">My blog</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="My-blog"><a href="#My-blog" class="headerlink" title="My blog"></a>My blog</h1>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/12/09/My-blog/" data-id="cjazf6zvu0001gfs67ghbriva" class="article-share-link">Compartir</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/决策树、随机森林、机器学习/">决策树、随机森林、机器学习</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Nube de Tags</h3>
    <div class="widget tagcloud">
      <a href="/tags/决策树、随机森林、机器学习/" style="font-size: 10px;">决策树、随机森林、机器学习</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archivos</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Posts recientes</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/12/09/决策树与随机森林/">决策树与随机森林</a>
          </li>
        
          <li>
            <a href="/2017/12/09/My-blog/">My blog</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 John Doe<br>
      Construido por <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>