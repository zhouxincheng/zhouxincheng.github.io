<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Hipanda'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Hexo</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?9996a598fab67114f38e4c027f3f0092";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/06/神经网络笔记-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/06/神经网络笔记-1/" itemprop="url">神经网络笔记-1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-06T16:58:41+08:00">
                2018-02-06
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/02/06/神经网络笔记-1/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/02/06/神经网络笔记-1/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Neural-Nets-notes-1"><a href="#Neural-Nets-notes-1" class="headerlink" title="Neural Nets notes 1"></a>Neural Nets notes 1</h1><p>内容结构：</p>
<ul>
<li>不用大脑类比进行简介</li>
<li><p>单神经元建模</p>
<ul>
<li>生物刺激和连接</li>
<li>单神经元线性分类器</li>
<li>常用的激活函数</li>
</ul>
</li>
<li><p>神经网络架构</p>
<ul>
<li>层的组织</li>
<li>前向反馈计算实例</li>
<li>变现的力量</li>
<li>设置层数和大小</li>
</ul>
</li>
<li><p>总结</p>
</li>
<li>附加的参考</li>
</ul>
<h2 id="快速介绍"><a href="#快速介绍" class="headerlink" title="快速介绍"></a>快速介绍</h2><p>不使用大脑类比来介绍神经网络。在线性分类器的章节中，我们通过为不同的类别计算得分分数, $s = Wx$,其中W是权重矩阵而x是输入图片的像素列向量。在CIFAR-10的列子中，x是维度为[3072<em>1]的列向量，W是[10</em>3072]的矩阵，所以输出的分数是10个类别分数的向量。</p>
<p>在神经网络中，通过$s = W_2<em>max(0,W_1x)$来计算分数。在这里W1可能是个[100</em>3072]的矩阵，将图片转换成100维的中间向量。max函数是非线性的函数，进行的是元素级运算。有许多的非线性函数可以供我们选择，但是max是最常见同时将所有激活函数值小于0归为0。W2矩阵的维度是[10*100]，因此最后我们将得到可以认为是10个类别的分数。应该注意的是非线性是计算上的一个关键点–如果我们不实用非线性函数，那么两个矩阵相乘则可以变为一个矩阵即可。由非线性函数我们可以得到“摆动”。可以通过链式求导规则和随机梯度下降法学习到参数W1和W2。</p>
<p>一个三层的神经网络可以类似的看成$s=W3max(0,W2max(0,W1x))$，其中$W3,W2,W1$是需要学习的参数。中间隐藏的向量的大小是网络的超参数，我们之后会学习如何设置他们。现在让我们看看如何从神经元/神经网络的角度解读这些计算。</p>
<h2 id="单神经元建模"><a href="#单神经元建模" class="headerlink" title="单神经元建模"></a>单神经元建模</h2><p>神经网络领域最初的主要灵感来自对生物神经系统进行建模的目标，但此后已经分化并成为工程问题，并在机器学习任务中取得了良好的结果。 尽管如此，我们还是从生物系统角度开始简短和高层次的讨论。</p>
<h3 id="生物动机和联系"><a href="#生物动机和联系" class="headerlink" title="生物动机和联系"></a>生物动机和联系</h3><p>大脑的基本计算单位是一个神经元。在人类神经系统中可以找到大约860亿个神经元，并且它们与大约$10^{14} - 10^{15}$个突触连接。下图显示了生物神经元的卡通图（左）和常见的数学模型（右）。每个神经元接收来自其树突的输入信号并沿着其（单个）轴突产生输出信号。轴突最终分出并通过突触连接到其他神经元的树突。在神经元的计算模型中，沿着轴突传播的信号（例如，x0）基于该突触（例如w0）处的突触强度，与另一神经元的树突相乘（例如，w0x0）。这个想法是，突触强度（权重w）是可以学习的，并且控制一个神经元对另一个神经元的影响力（和它的方向：兴奋（正面重量）或抑制（负面重量））。在基本模型中，树突将信号传送到细胞体，在那里它们都被累加起来。如果最后的总和超过一定的阈值，神经元可以触发，沿着它的轴突发出一个“尖峰”。在计算模型中，我们假设“尖峰”的准确时间不重要，只有射击的频率传达信息。基于这个速率代码的解释，我们模型神经元的激发率激活函数f，代表沿轴突的尖峰频率。从历史上看，激活函数的一个常见选择是sigmoid函数$σ$，因为它需要一个实值输入（和之后的信号强度），并将其约束为0到1之间的范围。稍后我们将会看到这些激活函数的细节。<br><img src="https://pic4.zhimg.com/80/d0cbce2f2654b8e70fe201fec2982c7d_hd.jpg" alt=""></p>
<p>单个神经元的前向传播示例代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class Neuron(object):</span><br><span class="line">  # ... </span><br><span class="line">  def forward(self, inputs):</span><br><span class="line">    &quot;&quot;&quot; assume inputs and weights are 1-D numpy arrays and bias is a number &quot;&quot;&quot;</span><br><span class="line">    cell_body_sum = np.sum(inputs * self.weights) + self.bias</span><br><span class="line">    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function</span><br><span class="line">    return firing_rate</span><br></pre></td></tr></table></figure>
<p>换句话说，每个神经元用输入及其权重执行一个点积，加上偏差并应用非线性（或激活函数），在这种情况下，sigmoid $σ（x）= 1 /（1 + e^{-x})$。 我们将在本节末尾详细介绍不同的激活功能。</p>
<p><em>粗模型</em>。 需要强调的是，这个生物神经元的模型是非常粗糙的：例如，有许多不同类型的神经元，每个神经元都有不同的属性。 生物神经元中的树突执行复杂的非线性计算。 突触不只是一个单一的权重，他们是一个复杂的非线性动力系统。已知许多系统中输出尖峰的确切时间是重要的，这表明速率代码近似可能不成立。由于所有这些和许多其他简化，如果您在神经网络和真正的大脑之间进行类比，请准备好听取来自具有神经科学背景的任何人的抱怨声。 如果您有兴趣，请参阅此<a href="https://physics.ucsd.edu/neurophysics/courses/physics_171/annurev.neuro.28.061604.135703.pdf" target="_blank" rel="noopener">评论</a>或最近的评论。</p>
<h3 id="单神经元作为线性分类器"><a href="#单神经元作为线性分类器" class="headerlink" title="单神经元作为线性分类器"></a>单神经元作为线性分类器</h3><p>神经元前向计算的数学形式可能看起来很熟悉。正如我们用线性分类器所看到的那样，神经元具有在其输入空间的某些线性区域“喜欢”（激活接近一个）或“不喜欢”（激活接近零）的能力。因此，在神经元输出上定义适当的损失函数的情况下，我们可以将单个神经元变成线性分类器：</p>
<p><strong>Binary Softmax classifier</strong>：例如，我们可以将$σ（Σ_iw_ix_i+ b）$解释为类$P（y_i = 1|x_i; w）$之一的概率。 另一类的概率是$P(yi = 0|x_i; w）= 1-P（y_i = 1|x_i; w）$，因为它们必须和为1。 通过这种解释，我们可以制定交叉熵损失，并且优化它将导致二元Softmax分类器（也被称为逻辑回归）。 由于sigmoid函数被限制在0-1之间，所以这个分类器的预测是基于神经元的输出是否大于0.5。</p>
<p><strong>二进制SVM分类器</strong>。 或者，我们可以在神经元的输出上附加一个hinge损失，并将其训练成二进制支持向量机。</p>
<p><strong>理解正则化</strong>。在SVM/Softmax的例子中，正则化损失从生物学角度可以看做逐渐遗忘，因为它的效果是让所有突触权重w在参数更新过程中逐渐向着0变化。</p>
<blockquote>
<p>一个单独的神经元可以用来实现一个二分类分类器，比如二分类的Softmax或者SVM分类器。</p>
</blockquote>
<h3 id="常用激活函数"><a href="#常用激活函数" class="headerlink" title="常用激活函数"></a>常用激活函数</h3><p>每个激活函数（或非线性函数）的输入都是一个数字，然后对其进行某种固定的数学操作。下面是在实践中可能遇到的几种激活函数：</p>
<p><img src="http://cs231n.github.io/assets/nn1/sigmoid.jpeg" alt=""></p>
<p><strong>$Sigmoid$</strong>: $σ(x)=1/(1+e^{−x})$，该函数接收一个实数并将其转变成0、1之间的值。特别的，当实数是很小的负数时函数值为0，为很大的正数时函数值为1，可以被解释为神经元的激活率，从压根不激活（0），至完全激活（1）。该激活函数在以前被经常使用，然而现在已经不使用了。主要有以下两个缺点：</p>
<ol>
<li>Sigmoid函数饱和使梯度消失。sigmoid神经元有一个不好的特性，就是当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0。意味着反向传播时，传递给之后的层的梯度接近0，进而杀死梯度，从而权重无法得到更新。还需要注意的一点是，为了防止神经元饱和，权重的矩阵的初始化不能过大，否则大多数神经元将过饱和，从而导致网络不学习。</li>
<li>Sigmoid的输出不是以0为均值的。这将影响之后的神经层，原因在于如果本层的神经元输出都为正（或者负），那么会导致梯度下降权重更新时出现z字型的下降，降低权重收敛的速度。虽然整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题。</li>
</ol>
<p><strong>$Tanh$</strong>:tanh神经元是一个简单放大的sigmoid神经元，$tanh(x)=2\sigma(2x)-1$，只是神经元输出变为[-1, 1]，这消除了sigmoid不以0为中心的特点，从而消除了sigmoid的第二个缺点。</p>
<p><img src="https://pic1.zhimg.com/80/83682a138f6224230f5b0292d9c01bd2_hd.jpg" alt=""></p>
<p>左边是ReLU（校正线性单元：Rectified Linear Unit）激活函数，当x=0时函数值为0。当x&gt;0函数的斜率为1。右边是从 Krizhevsky等的论文中截取的图表，指明使用ReLU比使用tanh的收敛快6倍。</p>
<p>$ReLU$:$f(x)=max(0,x)$近年来最常用的激活函数。使用ReLU的优缺点如下：</p>
<ol>
<li>相比于sigmoid和tanh函数，具有更快的收敛速度</li>
<li>计算简单，只是进行简单的阈值操作</li>
<li><strong>缺点</strong>：训练时，ReLU单元容易die。</li>
</ol>
<p>$Leaky ReLU$:$f(x)=𝟙(x<0)(αx)+𝟙(x>=0)(x)$，其中$\alpha$是一个小的常数值为解决ReLU死亡问题的尝试。</0)(αx)+𝟙(x></p>
<p><strong>Maxout</strong>:$max(w^T_1x+b_1,w^T_2x+b_2)$,是对ReLU和LeakyReLU的泛化。拥有ReLU所有的有点，而没有它的缺点。然而，神经元的参数数量Double。</p>
<p>以上是常用的激活函数，需要注意的是，在同一个网络中混合使用不同类型的神经元是非常少见的，虽然没有事实来禁止这样做。</p>
<blockquote>
<p>　总结：一般来说，使用ReLU激活函数，并设置好学习率，并可以监视网络中死亡的神经所占的比例。当神经元死亡过多时，可以试试Leaky ReLU和Maxout，但不要使用sigmoid函数。tanh可以一试，但效果一般不如ReLU和Maxout。</p>
</blockquote>
<h2 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h2><h3 id="灵活的组织层"><a href="#灵活的组织层" class="headerlink" title="灵活的组织层"></a>灵活的组织层</h3><p><strong>将神经网络算法以神经元的形式图形化</strong>。神经网络被建模成神经元的集合，神经元之间以无环图的形式进行连接。<br><img src="https://pic1.zhimg.com/80/ccb56c1fb267bc632d6d88459eb14ace_hd.jpg" alt=""></p>
<p><strong>命名规则</strong>。当我们说N层神经网络的时候，我们没有把输入层算入。因此，单层的神经网络就是没有隐层的（输入直接映射到输出）。因此，有的研究者会说逻辑回归或者SVM只是单层神经网络的一个特例。研究者们也会使用人工神经网络（Artificial Neural Networks 缩写ANN）或者多层感知器（Multi-Layer Perceptrons 缩写MLP）来指代神经网络。很多研究者并不喜欢神经网络算法和人类大脑之间的类比，他们更倾向于用单元（unit）而不是神经元作为术语。</p>
<p><strong>输出层</strong>。和神经网络中其他层不同，输出层的神经元一般是不会有激活函数的。这是因为最后的输出层大多用于表示分类评分值，因此是任意值的实数，或者某种实数值的目标数（比如在回归中）</p>
<p><strong>确定网络尺寸</strong>。用来度量神经网络的尺寸的标准主要有两个：一个是神经元的个数，另一个是参数的个数</p>
<h3 id="表现能力"><a href="#表现能力" class="headerlink" title="表现能力"></a>表现能力</h3><p>一种看待全连接的神经网络的方式是将其看成是由网络权重参数化的函数家族。一个很自然的问题便是，神经网络表示的函数家族有多复杂，即能表达的函数的种类有多少。特别的，是否存在不能被神经网络所建模的函数。</p>
<p>已经证明，具有一个隐藏层的神经网络可以近似表示任意连续函数。虽然一个两层的神经网络已经足够在数学理论上能完美地近似所有连续函数，但是在实际操作中效果相对较差。实际中，神经网络的表达出的函数不仅平滑，对于数据的统计特性有很好的拟合，同时可以通过优化函数求出该函数。理论上，单隐藏层和多隐藏层在表达能力上是一致的，但是经验上，深层神经网络效果更加优越。</p>
<p>另外，在实践中，3层神经网络通常比2层网络要好，但更深层（4,5,6层）的帮助很少。 这与卷积网络形成了鲜明的对比，在卷积神经网络中，已经发现深度对于良好的识别系统是非常重要的组成部分。 这种观察的一个论据是图像分层结构（例如，面部由眼睛组成，而眼睛由边缘等组成），因此对于图像识别领域来说，多层处理是直观的。</p>
<h3 id="设置层的数量和尺寸"><a href="#设置层的数量和尺寸" class="headerlink" title="设置层的数量和尺寸"></a>设置层的数量和尺寸</h3><p>在面临实际问题时，我们如何决定使用哪种架构？我们应该使用没有隐藏层？一个隐藏的层？还是两个隐藏层？每层应该多大？<br>首先，随着我们增加神经网络中的层数和层数，网络的容量会增加。 也就是说，可表示函数越来越复杂，因为神经元可以协作表达许多不同的功能。 例如，假设我们在二维中存在二元分类问题。 我们可以训练三个独立的神经网络，每个神经网络都有一个大小不一的隐藏层，并获得以下分类器：<br><img src="http://cs231n.github.io/assets/nn1/layer_sizes.jpeg" alt=""></p>
<p>在上图中，可以看见有更多神经元的神经网络可以表达更复杂的函数。然而这既是优势也是不足，优势是可以分类更复杂的数据，不足是可能造成对训练数据的过拟合。过拟合（Overfitting）是网络对数据中的噪声有很强的拟合能力，而没有重视数据间（假设）的潜在基本关系。</p>
<p>基于上面的讨论，看起来如果数据不是足够复杂，则似乎小一点的网络更好，因为可以防止过拟合。然而并非如此，防止神经网络的过拟合有很多方法（L2正则化，dropout和输入噪音等）。在实践中，使用这些方法来控制过拟合比减少网络神经元数目要好得多。</p>
<p>不要减少网络神经元数目的主要原因在于小网络更难使用梯度下降等局部方法来进行训练：虽然小型网络的损失函数的局部极小值更少，也比较容易收敛到这些局部极小值，但是这些最小值一般都很差，损失值很高。相反，大网络拥有更多的局部极小值，但就实际损失值来看，这些局部极小值表现更好，损失更小。因为神经网络是非凸的，就很难从数学上研究这些特性。即便如此，还是有一些文章尝试对这些目标函数进行理解，例如The Loss Surfaces of Multilayer Networks这篇论文。在实际中，你将发现如果训练的是一个小网络，那么最终的损失值将展现出多变性：某些情况下运气好会收敛到一个好的地方，某些情况下就收敛到一个不好的极值。从另一方面来说，如果你训练一个大的网络，你将发现许多不同的解决方法，但是最终损失值的差异将会小很多。这就是说，所有的解决办法都差不多，而且对于随机初始化参数好坏的依赖也会小很多。</p>
<blockquote>
<p>重申一下，正则化强度是控制神经网络过度拟合的首选方法。 我们可以看看三种不同设置的结果：<br><img src="http://cs231n.github.io/assets/nn1/reg_strengths.jpeg" alt=""></p>
<p>需要牢记于心的是：不应该因为担心出现过拟合而使用小网络。相反，应该进尽可能使用大网络，然后使用正则化技巧来控制过拟合。</p>
</blockquote>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul>
<li>介绍了一个非常粗糙的生物神经元模型</li>
<li>讨论了在实践中使用的几种类型的激活函数，其中ReLU是最常见的选择</li>
<li>介绍了神经网络，神经元通过全连接层连接，层间神经元两两相连，但是层内神经元不连接；</li>
<li>理解了分层的结构能够让神经网络高效地进行矩阵乘法和激活函数运算；</li>
<li>解释神经网络是一个通用函数近似器，我们也讨论了这个属性与它们无处不在的用处无关。它们被使用是因为它们对实践中出现的函数的功能形式做出了某些“正确”的假设。</li>
<li>讨论了更大网络总是更好的这一事实。然而更大容量的模型一定要和更强的正则化（比如更高的权重衰减）配合，否则它们就会过拟合。在后续章节中我们讲学习更多正则化的方法，尤其是dropout。</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://link.zhihu.com/?target=http%3A//www.deeplearning.net/tutorial/mlp.html" target="_blank" rel="noopener">deeplearning.net tutorial </a> with Theano</li>
<li><a href="http://cs.stanford.edu/people/karpathy/convnetjs/" target="_blank" rel="noopener">ConvNetJS demos for intuitions</a></li>
<li><a href="http://neuralnetworksanddeeplearning.com/chap1.html" target="_blank" rel="noopener">Michael Nielsen’s tutorials</a></li>
<li><a href="http://cs231n.github.io/neural-networks-1/" target="_blank" rel="noopener">原文链接：Neural Nets notes 1</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/04/Neural-Nets-Notes-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/04/Neural-Nets-Notes-1/" itemprop="url">Neural Nets Notes-1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-04T13:49:39+08:00">
                2018-02-04
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/02/04/Neural-Nets-Notes-1/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/02/04/Neural-Nets-Notes-1/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Neural-Nets-notes-1"><a href="#Neural-Nets-notes-1" class="headerlink" title="Neural Nets notes 1"></a>Neural Nets notes 1</h1><p>内容结构：</p>
<ul>
<li>不用大脑类比进行简介</li>
<li><p>单神经元建模</p>
<ul>
<li>生物刺激和连接</li>
<li>单神经元线性分类器</li>
<li>常用的激活函数</li>
</ul>
</li>
<li><p>神经网络架构</p>
<ul>
<li>层的组织</li>
<li>前向反馈计算实例</li>
<li>变现的力量</li>
<li>设置层数和大小</li>
</ul>
</li>
<li><p>总结</p>
</li>
<li>附加的参考</li>
</ul>
<h2 id="快速介绍"><a href="#快速介绍" class="headerlink" title="快速介绍"></a>快速介绍</h2><p>不使用大脑类比来介绍神经网络。在线性分类器的章节中，我们通过为不同的类别计算得分分数, $s = Wx$,其中W是权重矩阵而x是输入图片的像素列向量。在CIFAR-10的列子中，x是维度为[3072<em>1]的列向量，W是[10</em>3072]的矩阵，所以输出的分数是10个类别分数的向量。</p>
<p>在神经网络中，通过$s = W_2<em>max(0,W_1x)$来计算分数。在这里W1可能是个[100</em>3072]的矩阵，将图片转换成100维的中间向量。max函数是非线性的函数，进行的是元素级运算。有许多的非线性函数可以供我们选择，但是max是最常见同时将所有激活函数值小于0归为0。W2矩阵的维度是[10*100]，因此最后我们将得到可以认为是10个类别的分数。应该注意的是非线性是计算上的一个关键点–如果我们不实用非线性函数，那么两个矩阵相乘则可以变为一个矩阵即可。由非线性函数我们可以得到“摆动”。可以通过链式求导规则和随机梯度下降法学习到参数W1和W2。</p>
<p>一个三层的神经网络可以类似的看成$s=W3max(0,W2max(0,W1x))$，其中$W3,W2,W1$是需要学习的参数。中间隐藏的向量的大小是网络的超参数，我们之后会学习如何设置他们。现在让我们看看如何从神经元/神经网络的角度解读这些计算。</p>
<h2 id="单神经元建模"><a href="#单神经元建模" class="headerlink" title="单神经元建模"></a>单神经元建模</h2><p>神经网络领域最初的主要灵感来自对生物神经系统进行建模的目标，但此后已经分化并成为工程问题，并在机器学习任务中取得了良好的结果。 尽管如此，我们还是从生物系统角度开始简短和高层次的讨论。</p>
<h3 id="生物动机和联系"><a href="#生物动机和联系" class="headerlink" title="生物动机和联系"></a>生物动机和联系</h3><p>大脑的基本计算单位是一个神经元。在人类神经系统中可以找到大约860亿个神经元，并且它们与大约$10^{14} - 10^{15}$个突触连接。下图显示了生物神经元的卡通图（左）和常见的数学模型（右）。每个神经元接收来自其树突的输入信号并沿着其（单个）轴突产生输出信号。轴突最终分出并通过突触连接到其他神经元的树突。在神经元的计算模型中，沿着轴突传播的信号（例如，x0）基于该突触（例如w0）处的突触强度，与另一神经元的树突相乘（例如，w0x0）。这个想法是，突触强度（权重w）是可以学习的，并且控制一个神经元对另一个神经元的影响力（和它的方向：兴奋（正面重量）或抑制（负面重量））。在基本模型中，树突将信号传送到细胞体，在那里它们都被累加起来。如果最后的总和超过一定的阈值，神经元可以触发，沿着它的轴突发出一个“尖峰”。在计算模型中，我们假设“尖峰”的准确时间不重要，只有射击的频率传达信息。基于这个速率代码的解释，我们模型神经元的激发率激活函数f，代表沿轴突的尖峰频率。从历史上看，激活函数的一个常见选择是sigmoid函数$σ$，因为它需要一个实值输入（和之后的信号强度），并将其约束为0到1之间的范围。稍后我们将会看到这些激活函数的细节。<br><img src="https://pic4.zhimg.com/80/d0cbce2f2654b8e70fe201fec2982c7d_hd.jpg" alt=""></p>
<p>单个神经元的前向传播示例代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class Neuron(object):</span><br><span class="line">  # ... </span><br><span class="line">  def forward(self, inputs):</span><br><span class="line">    &quot;&quot;&quot; assume inputs and weights are 1-D numpy arrays and bias is a number &quot;&quot;&quot;</span><br><span class="line">    cell_body_sum = np.sum(inputs * self.weights) + self.bias</span><br><span class="line">    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function</span><br><span class="line">    return firing_rate</span><br></pre></td></tr></table></figure>
<p>换句话说，每个神经元用输入及其权重执行一个点积，加上偏差并应用非线性（或激活函数），在这种情况下，sigmoid $σ（x）= 1 /（1 + e^{-x})$。 我们将在本节末尾详细介绍不同的激活功能。</p>
<p><em>粗模型</em>。 需要强调的是，这个生物神经元的模型是非常粗糙的：例如，有许多不同类型的神经元，每个神经元都有不同的属性。 生物神经元中的树突执行复杂的非线性计算。 突触不只是一个单一的权重，他们是一个复杂的非线性动力系统。已知许多系统中输出尖峰的确切时间是重要的，这表明速率代码近似可能不成立。由于所有这些和许多其他简化，如果您在神经网络和真正的大脑之间进行类比，请准备好听取来自具有神经科学背景的任何人的抱怨声。 如果您有兴趣，请参阅此<a href="https://physics.ucsd.edu/neurophysics/courses/physics_171/annurev.neuro.28.061604.135703.pdf" target="_blank" rel="noopener">评论</a>或最近的评论。</p>
<h3 id="单神经元作为线性分类器"><a href="#单神经元作为线性分类器" class="headerlink" title="单神经元作为线性分类器"></a>单神经元作为线性分类器</h3><p>神经元前向计算的数学形式可能看起来很熟悉。正如我们用线性分类器所看到的那样，神经元具有在其输入空间的某些线性区域“喜欢”（激活接近一个）或“不喜欢”（激活接近零）的能力。因此，在神经元输出上定义适当的损失函数的情况下，我们可以将单个神经元变成线性分类器：</p>
<p><strong>Binary Softmax classifier</strong>：例如，我们可以将$σ（Σ_iw_ix_i+ b）$解释为类$P（y_i = 1|x_i; w）$之一的概率。 另一类的概率是$P(yi = 0|x_i; w）= 1-P（y_i = 1|x_i; w）$，因为它们必须和为1。 通过这种解释，我们可以制定交叉熵损失，并且优化它将导致二元Softmax分类器（也被称为逻辑回归）。 由于sigmoid函数被限制在0-1之间，所以这个分类器的预测是基于神经元的输出是否大于0.5。</p>
<p><strong>二进制SVM分类器</strong>。 或者，我们可以在神经元的输出上附加一个hinge损失，并将其训练成二进制支持向量机。</p>
<p><strong>理解正则化</strong>。在SVM/Softmax的例子中，正则化损失从生物学角度可以看做逐渐遗忘，因为它的效果是让所有突触权重w在参数更新过程中逐渐向着0变化。</p>
<blockquote>
<p>一个单独的神经元可以用来实现一个二分类分类器，比如二分类的Softmax或者SVM分类器。</p>
</blockquote>
<h3 id="常用激活函数"><a href="#常用激活函数" class="headerlink" title="常用激活函数"></a>常用激活函数</h3><p>每个激活函数（或非线性函数）的输入都是一个数字，然后对其进行某种固定的数学操作。下面是在实践中可能遇到的几种激活函数：</p>
<p><img src="http://cs231n.github.io/assets/nn1/sigmoid.jpeg" alt=""></p>
<p><strong>$Sigmoid$</strong>: $σ(x)=1/(1+e^{−x})$，该函数接收一个实数并将其转变成0、1之间的值。特别的，当实数是很小的负数时函数值为0，为很大的正数时函数值为1，可以被解释为神经元的激活率，从压根不激活（0），至完全激活（1）。该激活函数在以前被经常使用，然而现在已经不使用了。主要有以下两个缺点：</p>
<ol>
<li>Sigmoid函数饱和使梯度消失。sigmoid神经元有一个不好的特性，就是当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0。意味着反向传播时，传递给之后的层的梯度接近0，进而杀死梯度，从而权重无法得到更新。还需要注意的一点是，为了防止神经元饱和，权重的矩阵的初始化不能过大，否则大多数神经元将过饱和，从而导致网络不学习。</li>
<li>Sigmoid的输出不是以0为均值的。这将影响之后的神经层，原因在于如果本层的神经元输出都为正（或者负），那么会导致梯度下降权重更新时出现z字型的下降，降低权重收敛的速度。虽然整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题。</li>
</ol>
<p><strong>$Tanh$</strong>:tanh神经元是一个简单放大的sigmoid神经元，$tanh(x)=2\sigma(2x)-1$，只是神经元输出变为[-1, 1]，这消除了sigmoid不以0为中心的特点，从而消除了sigmoid的第二个缺点。</p>
<p><img src="https://pic1.zhimg.com/80/83682a138f6224230f5b0292d9c01bd2_hd.jpg" alt=""></p>
<p>左边是ReLU（校正线性单元：Rectified Linear Unit）激活函数，当x=0时函数值为0。当x&gt;0函数的斜率为1。右边是从 Krizhevsky等的论文中截取的图表，指明使用ReLU比使用tanh的收敛快6倍。</p>
<p>$ReLU$:$f(x)=max(0,x)$近年来最常用的激活函数。使用ReLU的优缺点如下：</p>
<ol>
<li>相比于sigmoid和tanh函数，具有更快的收敛速度</li>
<li>计算简单，只是进行简单的阈值操作</li>
<li><strong>缺点</strong>：训练时，ReLU单元容易die。</li>
</ol>
<p>$Leaky ReLU$:$f(x)=𝟙(x<0)(αx)+𝟙(x>=0)(x)$，其中$\alpha$是一个小的常数值为解决ReLU死亡问题的尝试。</0)(αx)+𝟙(x></p>
<p><strong>Maxout</strong>:$max(w^T_1x+b_1,w^T_2x+b_2)$,是对ReLU和LeakyReLU的泛化。拥有ReLU所有的有点，而没有它的缺点。然而，神经元的参数数量Double。</p>
<p>以上是常用的激活函数，需要注意的是，在同一个网络中混合使用不同类型的神经元是非常少见的，虽然没有事实来禁止这样做。</p>
<blockquote>
<p>　总结：一般来说，使用ReLU激活函数，并设置好学习率，并可以监视网络中死亡的神经所占的比例。当神经元死亡过多时，可以试试Leaky ReLU和Maxout，但不要使用sigmoid函数。tanh可以一试，但效果一般不如ReLU和Maxout。</p>
</blockquote>
<h2 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h2><h3 id="灵活的组织层"><a href="#灵活的组织层" class="headerlink" title="灵活的组织层"></a>灵活的组织层</h3><p><strong>将神经网络算法以神经元的形式图形化</strong>。神经网络被建模成神经元的集合，神经元之间以无环图的形式进行连接。<br><img src="https://pic1.zhimg.com/80/ccb56c1fb267bc632d6d88459eb14ace_hd.jpg" alt=""></p>
<p><strong>命名规则</strong>。当我们说N层神经网络的时候，我们没有把输入层算入。因此，单层的神经网络就是没有隐层的（输入直接映射到输出）。因此，有的研究者会说逻辑回归或者SVM只是单层神经网络的一个特例。研究者们也会使用人工神经网络（Artificial Neural Networks 缩写ANN）或者多层感知器（Multi-Layer Perceptrons 缩写MLP）来指代神经网络。很多研究者并不喜欢神经网络算法和人类大脑之间的类比，他们更倾向于用单元（unit）而不是神经元作为术语。</p>
<p><strong>输出层</strong>。和神经网络中其他层不同，输出层的神经元一般是不会有激活函数的。这是因为最后的输出层大多用于表示分类评分值，因此是任意值的实数，或者某种实数值的目标数（比如在回归中）</p>
<p><strong>确定网络尺寸</strong>。用来度量神经网络的尺寸的标准主要有两个：一个是神经元的个数，另一个是参数的个数</p>
<h3 id="表现能力"><a href="#表现能力" class="headerlink" title="表现能力"></a>表现能力</h3><p>一种看待全连接的神经网络的方式是将其看成是由网络权重参数化的函数家族。一个很自然的问题便是，神经网络表示的函数家族有多复杂，即能表达的函数的种类有多少。特别的，是否存在不能被神经网络所建模的函数。</p>
<p>已经证明，具有一个隐藏层的神经网络可以近似表示任意连续函数。虽然一个两层的神经网络已经足够在数学理论上能完美地近似所有连续函数，但是在实际操作中效果相对较差。实际中，神经网络的表达出的函数不仅平滑，对于数据的统计特性有很好的拟合，同时可以通过优化函数求出该函数。理论上，单隐藏层和多隐藏层在表达能力上是一致的，但是经验上，深层神经网络效果更加优越。</p>
<p>另外，在实践中，3层神经网络通常比2层网络要好，但更深层（4,5,6层）的帮助很少。 这与卷积网络形成了鲜明的对比，在卷积神经网络中，已经发现深度对于良好的识别系统是非常重要的组成部分。 这种观察的一个论据是图像分层结构（例如，面部由眼睛组成，而眼睛由边缘等组成），因此对于图像识别领域来说，多层处理是直观的。</p>
<h3 id="设置层的数量和尺寸"><a href="#设置层的数量和尺寸" class="headerlink" title="设置层的数量和尺寸"></a>设置层的数量和尺寸</h3><p>在面临实际问题时，我们如何决定使用哪种架构？我们应该使用没有隐藏层？一个隐藏的层？还是两个隐藏层？每层应该多大？<br>首先，随着我们增加神经网络中的层数和层数，网络的容量会增加。 也就是说，可表示函数越来越复杂，因为神经元可以协作表达许多不同的功能。 例如，假设我们在二维中存在二元分类问题。 我们可以训练三个独立的神经网络，每个神经网络都有一个大小不一的隐藏层，并获得以下分类器：<br><img src="http://cs231n.github.io/assets/nn1/layer_sizes.jpeg" alt=""></p>
<p>在上图中，可以看见有更多神经元的神经网络可以表达更复杂的函数。然而这既是优势也是不足，优势是可以分类更复杂的数据，不足是可能造成对训练数据的过拟合。过拟合（Overfitting）是网络对数据中的噪声有很强的拟合能力，而没有重视数据间（假设）的潜在基本关系。</p>
<p>基于上面的讨论，看起来如果数据不是足够复杂，则似乎小一点的网络更好，因为可以防止过拟合。然而并非如此，防止神经网络的过拟合有很多方法（L2正则化，dropout和输入噪音等）。在实践中，使用这些方法来控制过拟合比减少网络神经元数目要好得多。</p>
<p>不要减少网络神经元数目的主要原因在于小网络更难使用梯度下降等局部方法来进行训练：虽然小型网络的损失函数的局部极小值更少，也比较容易收敛到这些局部极小值，但是这些最小值一般都很差，损失值很高。相反，大网络拥有更多的局部极小值，但就实际损失值来看，这些局部极小值表现更好，损失更小。因为神经网络是非凸的，就很难从数学上研究这些特性。即便如此，还是有一些文章尝试对这些目标函数进行理解，例如The Loss Surfaces of Multilayer Networks这篇论文。在实际中，你将发现如果训练的是一个小网络，那么最终的损失值将展现出多变性：某些情况下运气好会收敛到一个好的地方，某些情况下就收敛到一个不好的极值。从另一方面来说，如果你训练一个大的网络，你将发现许多不同的解决方法，但是最终损失值的差异将会小很多。这就是说，所有的解决办法都差不多，而且对于随机初始化参数好坏的依赖也会小很多。</p>
<blockquote>
<p>重申一下，正则化强度是控制神经网络过度拟合的首选方法。 我们可以看看三种不同设置的结果：<br><img src="http://cs231n.github.io/assets/nn1/reg_strengths.jpeg" alt=""></p>
<p>需要牢记于心的是：不应该因为担心出现过拟合而使用小网络。相反，应该进尽可能使用大网络，然后使用正则化技巧来控制过拟合。</p>
</blockquote>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul>
<li>介绍了一个非常粗糙的生物神经元模型</li>
<li>讨论了在实践中使用的几种类型的激活函数，其中ReLU是最常见的选择</li>
<li>介绍了神经网络，神经元通过全连接层连接，层间神经元两两相连，但是层内神经元不连接；</li>
<li>理解了分层的结构能够让神经网络高效地进行矩阵乘法和激活函数运算；</li>
<li>解释神经网络是一个通用函数近似器，我们也讨论了这个属性与它们无处不在的用处无关。它们被使用是因为它们对实践中出现的函数的功能形式做出了某些“正确”的假设。</li>
<li>讨论了更大网络总是更好的这一事实。然而更大容量的模型一定要和更强的正则化（比如更高的权重衰减）配合，否则它们就会过拟合。在后续章节中我们讲学习更多正则化的方法，尤其是dropout。</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://link.zhihu.com/?target=http%3A//www.deeplearning.net/tutorial/mlp.html" target="_blank" rel="noopener">deeplearning.net tutorial </a> with Theano</li>
<li><a href="http://cs.stanford.edu/people/karpathy/convnetjs/" target="_blank" rel="noopener">ConvNetJS demos for intuitions</a></li>
<li><a href="http://neuralnetworksanddeeplearning.com/chap1.html" target="_blank" rel="noopener">Michael Nielsen’s tutorials</a></li>
<li><a href="http://cs231n.github.io/neural-networks-1/" target="_blank" rel="noopener">原文链接：Neural Nets notes 1</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/09/决策树与随机森林/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/09/决策树与随机森林/" itemprop="url">决策树与随机森林</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-12-09T22:03:57+08:00">
                2017-12-09
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/12/09/决策树与随机森林/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/12/09/决策树与随机森林/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-信息熵"><a href="#1-信息熵" class="headerlink" title="1. 信息熵"></a>1. 信息熵</h2><ul>
<li>熵：用以描述信息量的大小，当一个小概率事件发生时，它所蕴含的信息就是大的。公式：-$\sum_{i=1}^n$$p_i*log(p_i)$。</li>
<li>相对熵：又称互熵，设p(x)、q(x)是X中取值的两个概率分布，则p对q的相对熵是$$D(p||q) = \sum<em>x p(x)log{p(x) \over q(x)} = E</em>{p(x)}log{p(x) \over q(x)}$$；用于度量两个随机变量的距离.</li>
<li>联合熵：两个随机变量所包含的信息量</li>
<li>条件熵：$H(Y|X)=H(X,Y)-H(X)$,含义：(X,Y)发生所包含的熵，减去X单独发生时包含的熵，也可以表达为：在X发生的前提下，Y发生“新”带来的熵。定义式：$H(X,Y)-H(X)=-\sum<em>{x,y}p(x,y)logp(y|x)$ = $\sum</em>{x}p(x)H(Y|X=x)$（加权平均）</li>
<li>互信息：随机变量X与Y相互重叠的信息，形象化表示为维恩图中的X,Y交集部分。定义为X,Y的联合分布和独立分布乘积的相对熵。即：$$I(X,Y)=D(P(X,Y)||P(X)P(Y))$$</li>
<li>经验熵和经验条件熵：当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵。</li>
</ul>
<h2 id="2-决策树（Decision-Tree）"><a href="#2-决策树（Decision-Tree）" class="headerlink" title="2. 决策树（Decision Tree）"></a>2. 决策树（Decision Tree）</h2><h3 id="2-1-性质"><a href="#2-1-性质" class="headerlink" title="2.1 性质"></a>2.1 性质</h3><ol>
<li>树形结构，叶节点表示一种类别</li>
<li>以实例为基础的归纳学习</li>
<li>基本思想是以信息熵为度量构造熵值下降最快的树</li>
</ol>
<h3 id="2-2-特点"><a href="#2-2-特点" class="headerlink" title="2.2 特点"></a>2.2 特点</h3><ol>
<li>监督学习，自学习</li>
<li>从一类无序、无规则的事务中推理出分类规则</li>
</ol>
<h3 id="2-3-生成算法"><a href="#2-3-生成算法" class="headerlink" title="2.3 生成算法"></a>2.3 生成算法</h3><ul>
<li>ID3:Iterative Dichotomiser</li>
<li>C4.5</li>
<li>CART：Classification And Regression Tree</li>
</ul>
<h3 id="2-4-学习算法"><a href="#2-4-学习算法" class="headerlink" title="2.4 学习算法"></a>2.4 学习算法</h3><h4 id="2-4-1-信息增益"><a href="#2-4-1-信息增益" class="headerlink" title="2.4.1 信息增益"></a>2.4.1 信息增益</h4><ol>
<li>信息增益：$g(D|A)=H(D)-H(D|A)$–表示得知特征A的信息而使得X的信息的不确定减少的程度。从另一个角度来说：训练数据集与特征A的互信息</li>
<li>计算方法：<br>(1）先计算数据集D的经验熵$H(D)=-\sum<em>{k=1}^K\frac{|C</em>{k}|}{|D|}log\frac{|C_{k}|}{|D|}$<br>(2) 遍历所有特征，对于任一个特征A：先计算经验条件熵:$H(D|A)$，然后计算信息增益：$g(D,A)=H(D)-H(D|A)$，最后选择信息增益最大的特征作为当前分裂的特征</li>
</ol>
<h4 id="2-4-2-信息增益率"><a href="#2-4-2-信息增益率" class="headerlink" title="2.4.2 信息增益率"></a>2.4.2 信息增益率</h4><ol>
<li>信息增益率：$g_{r}(D,A)=g(D|A)/H(A)$</li>
<li>信息增益会导致特征数量越多的特征成为信息增益的首选特征，从而导致一些没有意义的特征被选取成增益特征，例如：编号。增益率解决了这个问题。</li>
</ol>
<h4 id="2-4-3-Gini系数"><a href="#2-4-3-Gini系数" class="headerlink" title="2.4.3 Gini系数"></a>2.4.3 Gini系数</h4><ol>
<li>Gini：$Gini(p)=\sum<em>{k=1}^{K}p</em>{k}(1-p<em>{k})=1-\sum</em>{k=1}^{K}p<em>{k}^{2}=1-\sum</em>{k=1}^{K}(\frac{|C_k|}{|D|})^2$</li>
<li>讨论：$H(X)=-\sum_{k=1}^{K}p_klnp<em>k\approx\sum</em>{k=1}^{K}p<em>{k}(1-p</em>{k})$</li>
<li>Gini系数的第二定义：经济邻域，表示贫富差距的衡量</li>
</ol>
<h4 id="2-4-4-学习算法总结"><a href="#2-4-4-学习算法总结" class="headerlink" title="2.4.4 学习算法总结"></a>2.4.4 学习算法总结</h4><ol>
<li>ID3：使用信息增益/互信息进行特征选择</li>
</ol>
<blockquote>
<p>取值多的属性，更容易使得数据更纯，其信息增益更大，极端例子：编号<br>最终得到的是一棵庞大但是深度浅的树，不合理</p>
</blockquote>
<ol>
<li>C4.5：信息增益率作为学习算法</li>
<li>CART:基尼系数</li>
</ol>
<h3 id="2-5-决策树的评价"><a href="#2-5-决策树的评价" class="headerlink" title="2.5 决策树的评价"></a>2.5 决策树的评价</h3><ol>
<li>损失函数：$C(T)=\sum<em>{t\in leaf}N</em>{t}H(t)$,对所有的叶节点的熵进行求和，该值越小越能说明对样本的分类越精确。</li>
<li><strong>各叶结点包含的样本数目不同，可使用样本数加权求熵和</strong></li>
</ol>
<h3 id="2-6-过拟合处理方法"><a href="#2-6-过拟合处理方法" class="headerlink" title="2.6 过拟合处理方法"></a>2.6 过拟合处理方法</h3><h4 id="2-6-1-剪枝"><a href="#2-6-1-剪枝" class="headerlink" title="2.6.1 剪枝"></a>2.6.1 剪枝</h4><blockquote>
<p>剪枝总体思路：</p>
<ol>
<li>由完全树$T_0$开始，剪枝部分节点得到$T$,再次剪枝部分节点得到$T$，直到仅剩根的树</li>
<li>在验证数据集上对这K个树分别评价，选择损失函数最小的树$T_\alpha$</li>
</ol>
</blockquote>
<ol>
<li>预剪枝：规定树的深度；设定熵的最小阈值；规定叶子节点的最少数量</li>
<li>后剪枝：选择最小的$\alpha$，对子树进行合并，迭代进行从而得到一系列的$T_0\;T_1\;T_2\cdot\cdot\cdot T_N$</li>
</ol>
<h5 id="2-6-1-1-后剪枝"><a href="#2-6-1-1-后剪枝" class="headerlink" title="2.6.1.1 后剪枝"></a>2.6.1.1 后剪枝</h5><ol>
<li>修正损失函数：$C(T)=\sum_{t\in leaf}N_t\cdot H(t)$</li>
<li>假定当前对$\gamma$为根的子树剪枝，剪枝后只保留节点本身而删除子树的所有节点</li>
<li>计算以$\gamma$为为根的子树，分别计算剪枝前后的损失函数，即$C<em>{\alpha}(r)=C(r)+\alpha$和$C</em>{\alpha}(R)=C(r)+\alpha\cdot|R<em>{leaf}|$；使两个等式相等求得$\alpha=\frac{C(r)-C(R)}{\langle R</em>{leaf}-1 \rangle}$。$\alpha$称为剪枝系数</li>
</ol>
<h5 id="2-6-1-2-剪枝算法"><a href="#2-6-1-2-剪枝算法" class="headerlink" title="2.6.1.2 剪枝算法"></a>2.6.1.2 剪枝算法</h5><ol>
<li>计算所有内部节点的剪枝系数</li>
<li>查找最小的剪枝系数的节点，剪枝得到决策树$T_k$；</li>
<li>重复以上步骤，直到得到只有一个节点的决策树</li>
<li>得到决策树序列$T_0\;T_1\;T_2\cdot\cdot\cdot T_N$</li>
<li>使用验证集选择最优子树</li>
</ol>
<hr>
<h2 id="3-随机森林"><a href="#3-随机森林" class="headerlink" title="3 随机森林"></a>3 随机森林</h2><h3 id="3-1-Bagging"><a href="#3-1-Bagging" class="headerlink" title="3.1 Bagging"></a>3.1 Bagging</h3><ol>
<li>从样本集有放回重复采样n个，m次</li>
<li>使用这n个样本建立m个分类器</li>
<li>输入数据，根据m个投票结果决定数据是哪一类型</li>
</ol>
<h3 id="3-2-随机森林-–-随机样本，随机属性"><a href="#3-2-随机森林-–-随机样本，随机属性" class="headerlink" title="3.2 随机森林 – 随机样本，随机属性"></a>3.2 随机森林 – 随机样本，随机属性</h3><ol>
<li>首先进行Bootstrap采样n个样本，总共进行m次</li>
<li>然后从所有属性里头选取k个属性，分别建立m棵决策树</li>
<li>最后，投票表决</li>
</ol>
<h3 id="3-3-随机森林-Bagging和决策树的关系"><a href="#3-3-随机森林-Bagging和决策树的关系" class="headerlink" title="3.3 随机森林/Bagging和决策树的关系"></a>3.3 随机森林/Bagging和决策树的关系</h3><ol>
<li>随机森林使用决策树作为基本分类器</li>
<li>也可以使用其他弱分类器组合成随机森林</li>
</ol>
<h2 id="4-样本不均衡的常用处理方法"><a href="#4-样本不均衡的常用处理方法" class="headerlink" title="4 样本不均衡的常用处理方法"></a>4 样本不均衡的常用处理方法</h2><p><strong>假定A类数量大于B类，且严重不均衡</strong></p>
<h3 id="4-1-A类欠采样Undersampling"><a href="#4-1-A类欠采样Undersampling" class="headerlink" title="4.1 A类欠采样Undersampling"></a>4.1 A类欠采样Undersampling</h3><ul>
<li>随机欠采样</li>
<li>A类分成若干子类</li>
<li>采用聚类对A类进行分割</li>
</ul>
<h3 id="4-2-B类过采样Oversampling"><a href="#4-2-B类过采样Oversampling" class="headerlink" title="4.2 B类过采样Oversampling"></a>4.2 B类过采样Oversampling</h3><blockquote>
<p>避免欠采样造成的信息丢失</p>
</blockquote>
<h3 id="4-3-B类数据合成Synthetic-Data-Generation"><a href="#4-3-B类数据合成Synthetic-Data-Generation" class="headerlink" title="4.3 B类数据合成Synthetic Data Generation"></a>4.3 B类数据合成Synthetic Data Generation</h3><ul>
<li>随机插值得到新样本</li>
<li>SMOTE</li>
</ul>
<h3 id="4-4-代价敏感学习"><a href="#4-4-代价敏感学习" class="headerlink" title="4.4 代价敏感学习"></a>4.4 代价敏感学习</h3><blockquote>
<p>降低A类权值，提高B类权值</p>
</blockquote>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><ol>
<li>决策树/随机森林逻辑简单，符合人类做判断的逻辑</li>
<li>随机森林的集大智（aggregation）的思想，可以用在其他分类器的设计中</li>
<li>数据的处理过程中需要考虑<strong>数据平衡</strong>问题</li>
</ol>
<h2 id="6-思考题"><a href="#6-思考题" class="headerlink" title="6. 思考题"></a>6. 思考题</h2><ol>
<li>思考随机森林为何可以提高正确率，且降低过拟合程度？</li>
</ol>
<blockquote>
<p>答：从概率学角度来说，多个弱分类器进行叠加使用会使得分类正确的可能性大大提升。随机森林综合考虑多个决策树，所以从一定程度上可以降低拟合程度</p>
</blockquote>
<ol>
<li>决策树后剪枝可以怎么操作？</li>
</ol>
<blockquote>
<p>答：计算决策树内部节点的剪枝系数，从小到大依次进行剪枝得到对应k棵树，最后使用验证集选取经验熵最小的那棵树木</p>
</blockquote>
<ol>
<li>请解释系数为何可以用于分类标准。</li>
</ol>
<blockquote>
<p>Gini系数从公式上类似于熵，可以看成是熵的系数在x=1处的一阶泰勒展开</p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/09/My-blog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/09/My-blog/" itemprop="url">My blog</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-12-09T22:00:19+08:00">
                2017-12-09
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/12/09/My-blog/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/12/09/My-blog/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="My-blog"><a href="#My-blog" class="headerlink" title="My blog"></a>My blog</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/uploads/avatar.png"
                alt="John Doe" />
            
              <p class="site-author-name" itemprop="name">John Doe</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">Artikel</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">Tags</span>
                
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhouxincheng" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zhouxincheng@pku.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://plus.google.com/John David" target="_blank" title="Google">
                      
                        <i class="fa fa-fw fa-google"></i>Google</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/zhou-xin-cheng-1" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-globe"></i>知乎</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2016 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  
</div>


  <div class="powered-by">Erstellt mit  <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://Jason.disqus.com/count.js" async></script>
    

    

  




	





  














  





  

  

  

  
  

  

  

  

</body>
</html>
