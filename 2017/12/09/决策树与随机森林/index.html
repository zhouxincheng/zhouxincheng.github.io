<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="决策树、随机森林、机器学习," />










<meta name="description" content="1. 信息熵 熵：用以描述信息量的大小，当一个小概率事件发生时，它所蕴含的信息就是大的。公式：-$\sum_{i=1}^n$$p_i*log(p_i)$。 相对熵：又称互熵，设p(x)、q(x)是X中取值的两个概率分布，则p对q的相对熵是$$D(p||q) = \sumx p(x)log{p(x) \over q(x)} = E{p(x)}log{p(x) \over q(x)}$$；用于度量两个">
<meta name="keywords" content="决策树、随机森林、机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="决策树与随机森林">
<meta property="og:url" content="http://yoursite.com/2017/12/09/决策树与随机森林/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="1. 信息熵 熵：用以描述信息量的大小，当一个小概率事件发生时，它所蕴含的信息就是大的。公式：-$\sum_{i=1}^n$$p_i*log(p_i)$。 相对熵：又称互熵，设p(x)、q(x)是X中取值的两个概率分布，则p对q的相对熵是$$D(p||q) = \sumx p(x)log{p(x) \over q(x)} = E{p(x)}log{p(x) \over q(x)}$$；用于度量两个">
<meta property="og:updated_time" content="2018-01-27T17:02:24.522Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="决策树与随机森林">
<meta name="twitter:description" content="1. 信息熵 熵：用以描述信息量的大小，当一个小概率事件发生时，它所蕴含的信息就是大的。公式：-$\sum_{i=1}^n$$p_i*log(p_i)$。 相对熵：又称互熵，设p(x)、q(x)是X中取值的两个概率分布，则p对q的相对熵是$$D(p||q) = \sumx p(x)log{p(x) \over q(x)} = E{p(x)}log{p(x) \over q(x)}$$；用于度量两个">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Hipanda'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/12/09/决策树与随机森林/"/>





  <title>决策树与随机森林 | Hexo</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?9996a598fab67114f38e4c027f3f0092";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/09/决策树与随机森林/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">决策树与随机森林</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-12-09T22:03:57+08:00">
                2017-12-09
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/12/09/决策树与随机森林/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/12/09/决策树与随机森林/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="1-信息熵"><a href="#1-信息熵" class="headerlink" title="1. 信息熵"></a>1. 信息熵</h2><ul>
<li>熵：用以描述信息量的大小，当一个小概率事件发生时，它所蕴含的信息就是大的。公式：-$\sum_{i=1}^n$$p_i*log(p_i)$。</li>
<li>相对熵：又称互熵，设p(x)、q(x)是X中取值的两个概率分布，则p对q的相对熵是$$D(p||q) = \sum<em>x p(x)log{p(x) \over q(x)} = E</em>{p(x)}log{p(x) \over q(x)}$$；用于度量两个随机变量的距离.</li>
<li>联合熵：两个随机变量所包含的信息量</li>
<li>条件熵：$H(Y|X)=H(X,Y)-H(X)$,含义：(X,Y)发生所包含的熵，减去X单独发生时包含的熵，也可以表达为：在X发生的前提下，Y发生“新”带来的熵。定义式：$H(X,Y)-H(X)=-\sum<em>{x,y}p(x,y)logp(y|x)$ = $\sum</em>{x}p(x)H(Y|X=x)$（加权平均）</li>
<li>互信息：随机变量X与Y相互重叠的信息，形象化表示为维恩图中的X,Y交集部分。定义为X,Y的联合分布和独立分布乘积的相对熵。即：$$I(X,Y)=D(P(X,Y)||P(X)P(Y))$$</li>
<li>经验熵和经验条件熵：当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵。</li>
</ul>
<h2 id="2-决策树（Decision-Tree）"><a href="#2-决策树（Decision-Tree）" class="headerlink" title="2. 决策树（Decision Tree）"></a>2. 决策树（Decision Tree）</h2><h3 id="2-1-性质"><a href="#2-1-性质" class="headerlink" title="2.1 性质"></a>2.1 性质</h3><ol>
<li>树形结构，叶节点表示一种类别</li>
<li>以实例为基础的归纳学习</li>
<li>基本思想是以信息熵为度量构造熵值下降最快的树</li>
</ol>
<h3 id="2-2-特点"><a href="#2-2-特点" class="headerlink" title="2.2 特点"></a>2.2 特点</h3><ol>
<li>监督学习，自学习</li>
<li>从一类无序、无规则的事务中推理出分类规则</li>
</ol>
<h3 id="2-3-生成算法"><a href="#2-3-生成算法" class="headerlink" title="2.3 生成算法"></a>2.3 生成算法</h3><ul>
<li>ID3:Iterative Dichotomiser</li>
<li>C4.5</li>
<li>CART：Classification And Regression Tree</li>
</ul>
<h3 id="2-4-学习算法"><a href="#2-4-学习算法" class="headerlink" title="2.4 学习算法"></a>2.4 学习算法</h3><h4 id="2-4-1-信息增益"><a href="#2-4-1-信息增益" class="headerlink" title="2.4.1 信息增益"></a>2.4.1 信息增益</h4><ol>
<li>信息增益：$g(D|A)=H(D)-H(D|A)$–表示得知特征A的信息而使得X的信息的不确定减少的程度。从另一个角度来说：训练数据集与特征A的互信息</li>
<li>计算方法：<br>(1）先计算数据集D的经验熵$H(D)=-\sum<em>{k=1}^K\frac{|C</em>{k}|}{|D|}log\frac{|C_{k}|}{|D|}$<br>(2) 遍历所有特征，对于任一个特征A：先计算经验条件熵:$H(D|A)$，然后计算信息增益：$g(D,A)=H(D)-H(D|A)$，最后选择信息增益最大的特征作为当前分裂的特征</li>
</ol>
<h4 id="2-4-2-信息增益率"><a href="#2-4-2-信息增益率" class="headerlink" title="2.4.2 信息增益率"></a>2.4.2 信息增益率</h4><ol>
<li>信息增益率：$g_{r}(D,A)=g(D|A)/H(A)$</li>
<li>信息增益会导致特征数量越多的特征成为信息增益的首选特征，从而导致一些没有意义的特征被选取成增益特征，例如：编号。增益率解决了这个问题。</li>
</ol>
<h4 id="2-4-3-Gini系数"><a href="#2-4-3-Gini系数" class="headerlink" title="2.4.3 Gini系数"></a>2.4.3 Gini系数</h4><ol>
<li>Gini：$Gini(p)=\sum<em>{k=1}^{K}p</em>{k}(1-p<em>{k})=1-\sum</em>{k=1}^{K}p<em>{k}^{2}=1-\sum</em>{k=1}^{K}(\frac{|C_k|}{|D|})^2$</li>
<li>讨论：$H(X)=-\sum_{k=1}^{K}p_klnp<em>k\approx\sum</em>{k=1}^{K}p<em>{k}(1-p</em>{k})$</li>
<li>Gini系数的第二定义：经济邻域，表示贫富差距的衡量</li>
</ol>
<h4 id="2-4-4-学习算法总结"><a href="#2-4-4-学习算法总结" class="headerlink" title="2.4.4 学习算法总结"></a>2.4.4 学习算法总结</h4><ol>
<li>ID3：使用信息增益/互信息进行特征选择</li>
</ol>
<blockquote>
<p>取值多的属性，更容易使得数据更纯，其信息增益更大，极端例子：编号<br>最终得到的是一棵庞大但是深度浅的树，不合理</p>
</blockquote>
<ol>
<li>C4.5：信息增益率作为学习算法</li>
<li>CART:基尼系数</li>
</ol>
<h3 id="2-5-决策树的评价"><a href="#2-5-决策树的评价" class="headerlink" title="2.5 决策树的评价"></a>2.5 决策树的评价</h3><ol>
<li>损失函数：$C(T)=\sum<em>{t\in leaf}N</em>{t}H(t)$,对所有的叶节点的熵进行求和，该值越小越能说明对样本的分类越精确。</li>
<li><strong>各叶结点包含的样本数目不同，可使用样本数加权求熵和</strong></li>
</ol>
<h3 id="2-6-过拟合处理方法"><a href="#2-6-过拟合处理方法" class="headerlink" title="2.6 过拟合处理方法"></a>2.6 过拟合处理方法</h3><h4 id="2-6-1-剪枝"><a href="#2-6-1-剪枝" class="headerlink" title="2.6.1 剪枝"></a>2.6.1 剪枝</h4><blockquote>
<p>剪枝总体思路：</p>
<ol>
<li>由完全树$T_0$开始，剪枝部分节点得到$T$,再次剪枝部分节点得到$T$，直到仅剩根的树</li>
<li>在验证数据集上对这K个树分别评价，选择损失函数最小的树$T_\alpha$</li>
</ol>
</blockquote>
<ol>
<li>预剪枝：规定树的深度；设定熵的最小阈值；规定叶子节点的最少数量</li>
<li>后剪枝：选择最小的$\alpha$，对子树进行合并，迭代进行从而得到一系列的$T_0\;T_1\;T_2\cdot\cdot\cdot T_N$</li>
</ol>
<h5 id="2-6-1-1-后剪枝"><a href="#2-6-1-1-后剪枝" class="headerlink" title="2.6.1.1 后剪枝"></a>2.6.1.1 后剪枝</h5><ol>
<li>修正损失函数：$C(T)=\sum_{t\in leaf}N_t\cdot H(t)$</li>
<li>假定当前对$\gamma$为根的子树剪枝，剪枝后只保留节点本身而删除子树的所有节点</li>
<li>计算以$\gamma$为为根的子树，分别计算剪枝前后的损失函数，即$C<em>{\alpha}(r)=C(r)+\alpha$和$C</em>{\alpha}(R)=C(r)+\alpha\cdot|R<em>{leaf}|$；使两个等式相等求得$\alpha=\frac{C(r)-C(R)}{\langle R</em>{leaf}-1 \rangle}$。$\alpha$称为剪枝系数</li>
</ol>
<h5 id="2-6-1-2-剪枝算法"><a href="#2-6-1-2-剪枝算法" class="headerlink" title="2.6.1.2 剪枝算法"></a>2.6.1.2 剪枝算法</h5><ol>
<li>计算所有内部节点的剪枝系数</li>
<li>查找最小的剪枝系数的节点，剪枝得到决策树$T_k$；</li>
<li>重复以上步骤，直到得到只有一个节点的决策树</li>
<li>得到决策树序列$T_0\;T_1\;T_2\cdot\cdot\cdot T_N$</li>
<li>使用验证集选择最优子树</li>
</ol>
<hr>
<h2 id="3-随机森林"><a href="#3-随机森林" class="headerlink" title="3 随机森林"></a>3 随机森林</h2><h3 id="3-1-Bagging"><a href="#3-1-Bagging" class="headerlink" title="3.1 Bagging"></a>3.1 Bagging</h3><ol>
<li>从样本集有放回重复采样n个，m次</li>
<li>使用这n个样本建立m个分类器</li>
<li>输入数据，根据m个投票结果决定数据是哪一类型</li>
</ol>
<h3 id="3-2-随机森林-–-随机样本，随机属性"><a href="#3-2-随机森林-–-随机样本，随机属性" class="headerlink" title="3.2 随机森林 – 随机样本，随机属性"></a>3.2 随机森林 – 随机样本，随机属性</h3><ol>
<li>首先进行Bootstrap采样n个样本，总共进行m次</li>
<li>然后从所有属性里头选取k个属性，分别建立m棵决策树</li>
<li>最后，投票表决</li>
</ol>
<h3 id="3-3-随机森林-Bagging和决策树的关系"><a href="#3-3-随机森林-Bagging和决策树的关系" class="headerlink" title="3.3 随机森林/Bagging和决策树的关系"></a>3.3 随机森林/Bagging和决策树的关系</h3><ol>
<li>随机森林使用决策树作为基本分类器</li>
<li>也可以使用其他弱分类器组合成随机森林</li>
</ol>
<h2 id="4-样本不均衡的常用处理方法"><a href="#4-样本不均衡的常用处理方法" class="headerlink" title="4 样本不均衡的常用处理方法"></a>4 样本不均衡的常用处理方法</h2><p><strong>假定A类数量大于B类，且严重不均衡</strong></p>
<h3 id="4-1-A类欠采样Undersampling"><a href="#4-1-A类欠采样Undersampling" class="headerlink" title="4.1 A类欠采样Undersampling"></a>4.1 A类欠采样Undersampling</h3><ul>
<li>随机欠采样</li>
<li>A类分成若干子类</li>
<li>采用聚类对A类进行分割</li>
</ul>
<h3 id="4-2-B类过采样Oversampling"><a href="#4-2-B类过采样Oversampling" class="headerlink" title="4.2 B类过采样Oversampling"></a>4.2 B类过采样Oversampling</h3><blockquote>
<p>避免欠采样造成的信息丢失</p>
</blockquote>
<h3 id="4-3-B类数据合成Synthetic-Data-Generation"><a href="#4-3-B类数据合成Synthetic-Data-Generation" class="headerlink" title="4.3 B类数据合成Synthetic Data Generation"></a>4.3 B类数据合成Synthetic Data Generation</h3><ul>
<li>随机插值得到新样本</li>
<li>SMOTE</li>
</ul>
<h3 id="4-4-代价敏感学习"><a href="#4-4-代价敏感学习" class="headerlink" title="4.4 代价敏感学习"></a>4.4 代价敏感学习</h3><blockquote>
<p>降低A类权值，提高B类权值</p>
</blockquote>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><ol>
<li>决策树/随机森林逻辑简单，符合人类做判断的逻辑</li>
<li>随机森林的集大智（aggregation）的思想，可以用在其他分类器的设计中</li>
<li>数据的处理过程中需要考虑<strong>数据平衡</strong>问题</li>
</ol>
<h2 id="6-思考题"><a href="#6-思考题" class="headerlink" title="6. 思考题"></a>6. 思考题</h2><ol>
<li>思考随机森林为何可以提高正确率，且降低过拟合程度？</li>
</ol>
<blockquote>
<p>答：从概率学角度来说，多个弱分类器进行叠加使用会使得分类正确的可能性大大提升。随机森林综合考虑多个决策树，所以从一定程度上可以降低拟合程度</p>
</blockquote>
<ol>
<li>决策树后剪枝可以怎么操作？</li>
</ol>
<blockquote>
<p>答：计算决策树内部节点的剪枝系数，从小到大依次进行剪枝得到对应k棵树，最后使用验证集选取经验熵最小的那棵树木</p>
</blockquote>
<ol>
<li>请解释系数为何可以用于分类标准。</li>
</ol>
<blockquote>
<p>Gini系数从公式上类似于熵，可以看成是熵的系数在x=1处的一阶泰勒展开</p>
</blockquote>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/决策树、随机森林、机器学习/" rel="tag"># 决策树、随机森林、机器学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/12/09/My-blog/" rel="next" title="My blog">
                <i class="fa fa-chevron-left"></i> My blog
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/02/06/神经网络笔记-1/" rel="prev" title="神经网络笔记-1">
                神经网络笔记-1 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/uploads/avatar.png"
                alt="John Doe" />
            
              <p class="site-author-name" itemprop="name">John Doe</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhouxincheng" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zhouxincheng@pku.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://plus.google.com/John David" target="_blank" title="Google">
                      
                        <i class="fa fa-fw fa-google"></i>Google</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/zhou-xin-cheng-1" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-globe"></i>知乎</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-信息熵"><span class="nav-number">1.</span> <span class="nav-text"><a href="#1-&#x4FE1;&#x606F;&#x71B5;" class="headerlink" title="1. &#x4FE1;&#x606F;&#x71B5;"></a>1. &#x4FE1;&#x606F;&#x71B5;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-决策树（Decision-Tree）"><span class="nav-number">2.</span> <span class="nav-text"><a href="#2-&#x51B3;&#x7B56;&#x6811;&#xFF08;Decision-Tree&#xFF09;" class="headerlink" title="2. &#x51B3;&#x7B56;&#x6811;&#xFF08;Decision Tree&#xFF09;"></a>2. &#x51B3;&#x7B56;&#x6811;&#xFF08;Decision Tree&#xFF09;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-性质"><span class="nav-number">2.1.</span> <span class="nav-text"><a href="#2-1-&#x6027;&#x8D28;" class="headerlink" title="2.1 &#x6027;&#x8D28;"></a>2.1 &#x6027;&#x8D28;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-特点"><span class="nav-number">2.2.</span> <span class="nav-text"><a href="#2-2-&#x7279;&#x70B9;" class="headerlink" title="2.2 &#x7279;&#x70B9;"></a>2.2 &#x7279;&#x70B9;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-生成算法"><span class="nav-number">2.3.</span> <span class="nav-text"><a href="#2-3-&#x751F;&#x6210;&#x7B97;&#x6CD5;" class="headerlink" title="2.3 &#x751F;&#x6210;&#x7B97;&#x6CD5;"></a>2.3 &#x751F;&#x6210;&#x7B97;&#x6CD5;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-学习算法"><span class="nav-number">2.4.</span> <span class="nav-text"><a href="#2-4-&#x5B66;&#x4E60;&#x7B97;&#x6CD5;" class="headerlink" title="2.4 &#x5B66;&#x4E60;&#x7B97;&#x6CD5;"></a>2.4 &#x5B66;&#x4E60;&#x7B97;&#x6CD5;</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-1-信息增益"><span class="nav-number">2.4.1.</span> <span class="nav-text"><a href="#2-4-1-&#x4FE1;&#x606F;&#x589E;&#x76CA;" class="headerlink" title="2.4.1 &#x4FE1;&#x606F;&#x589E;&#x76CA;"></a>2.4.1 &#x4FE1;&#x606F;&#x589E;&#x76CA;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-2-信息增益率"><span class="nav-number">2.4.2.</span> <span class="nav-text"><a href="#2-4-2-&#x4FE1;&#x606F;&#x589E;&#x76CA;&#x7387;" class="headerlink" title="2.4.2 &#x4FE1;&#x606F;&#x589E;&#x76CA;&#x7387;"></a>2.4.2 &#x4FE1;&#x606F;&#x589E;&#x76CA;&#x7387;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-3-Gini系数"><span class="nav-number">2.4.3.</span> <span class="nav-text"><a href="#2-4-3-Gini&#x7CFB;&#x6570;" class="headerlink" title="2.4.3 Gini&#x7CFB;&#x6570;"></a>2.4.3 Gini&#x7CFB;&#x6570;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-4-学习算法总结"><span class="nav-number">2.4.4.</span> <span class="nav-text"><a href="#2-4-4-&#x5B66;&#x4E60;&#x7B97;&#x6CD5;&#x603B;&#x7ED3;" class="headerlink" title="2.4.4 &#x5B66;&#x4E60;&#x7B97;&#x6CD5;&#x603B;&#x7ED3;"></a>2.4.4 &#x5B66;&#x4E60;&#x7B97;&#x6CD5;&#x603B;&#x7ED3;</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-决策树的评价"><span class="nav-number">2.5.</span> <span class="nav-text"><a href="#2-5-&#x51B3;&#x7B56;&#x6811;&#x7684;&#x8BC4;&#x4EF7;" class="headerlink" title="2.5 &#x51B3;&#x7B56;&#x6811;&#x7684;&#x8BC4;&#x4EF7;"></a>2.5 &#x51B3;&#x7B56;&#x6811;&#x7684;&#x8BC4;&#x4EF7;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-过拟合处理方法"><span class="nav-number">2.6.</span> <span class="nav-text"><a href="#2-6-&#x8FC7;&#x62DF;&#x5408;&#x5904;&#x7406;&#x65B9;&#x6CD5;" class="headerlink" title="2.6 &#x8FC7;&#x62DF;&#x5408;&#x5904;&#x7406;&#x65B9;&#x6CD5;"></a>2.6 &#x8FC7;&#x62DF;&#x5408;&#x5904;&#x7406;&#x65B9;&#x6CD5;</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-6-1-剪枝"><span class="nav-number">2.6.1.</span> <span class="nav-text"><a href="#2-6-1-&#x526A;&#x679D;" class="headerlink" title="2.6.1 &#x526A;&#x679D;"></a>2.6.1 &#x526A;&#x679D;</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-6-1-1-后剪枝"><span class="nav-number">2.6.1.1.</span> <span class="nav-text"><a href="#2-6-1-1-&#x540E;&#x526A;&#x679D;" class="headerlink" title="2.6.1.1 &#x540E;&#x526A;&#x679D;"></a>2.6.1.1 &#x540E;&#x526A;&#x679D;</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-6-1-2-剪枝算法"><span class="nav-number">2.6.1.2.</span> <span class="nav-text"><a href="#2-6-1-2-&#x526A;&#x679D;&#x7B97;&#x6CD5;" class="headerlink" title="2.6.1.2 &#x526A;&#x679D;&#x7B97;&#x6CD5;"></a>2.6.1.2 &#x526A;&#x679D;&#x7B97;&#x6CD5;</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-随机森林"><span class="nav-number">3.</span> <span class="nav-text"><a href="#3-&#x968F;&#x673A;&#x68EE;&#x6797;" class="headerlink" title="3 &#x968F;&#x673A;&#x68EE;&#x6797;"></a>3 &#x968F;&#x673A;&#x68EE;&#x6797;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Bagging"><span class="nav-number">3.1.</span> <span class="nav-text"><a href="#3-1-Bagging" class="headerlink" title="3.1 Bagging"></a>3.1 Bagging</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-随机森林-–-随机样本，随机属性"><span class="nav-number">3.2.</span> <span class="nav-text"><a href="#3-2-&#x968F;&#x673A;&#x68EE;&#x6797;-&#x2013;-&#x968F;&#x673A;&#x6837;&#x672C;&#xFF0C;&#x968F;&#x673A;&#x5C5E;&#x6027;" class="headerlink" title="3.2 &#x968F;&#x673A;&#x68EE;&#x6797; &#x2013; &#x968F;&#x673A;&#x6837;&#x672C;&#xFF0C;&#x968F;&#x673A;&#x5C5E;&#x6027;"></a>3.2 &#x968F;&#x673A;&#x68EE;&#x6797; &#x2013; &#x968F;&#x673A;&#x6837;&#x672C;&#xFF0C;&#x968F;&#x673A;&#x5C5E;&#x6027;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-随机森林-Bagging和决策树的关系"><span class="nav-number">3.3.</span> <span class="nav-text"><a href="#3-3-&#x968F;&#x673A;&#x68EE;&#x6797;-Bagging&#x548C;&#x51B3;&#x7B56;&#x6811;&#x7684;&#x5173;&#x7CFB;" class="headerlink" title="3.3 &#x968F;&#x673A;&#x68EE;&#x6797;/Bagging&#x548C;&#x51B3;&#x7B56;&#x6811;&#x7684;&#x5173;&#x7CFB;"></a>3.3 &#x968F;&#x673A;&#x68EE;&#x6797;/Bagging&#x548C;&#x51B3;&#x7B56;&#x6811;&#x7684;&#x5173;&#x7CFB;</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-样本不均衡的常用处理方法"><span class="nav-number">4.</span> <span class="nav-text"><a href="#4-&#x6837;&#x672C;&#x4E0D;&#x5747;&#x8861;&#x7684;&#x5E38;&#x7528;&#x5904;&#x7406;&#x65B9;&#x6CD5;" class="headerlink" title="4 &#x6837;&#x672C;&#x4E0D;&#x5747;&#x8861;&#x7684;&#x5E38;&#x7528;&#x5904;&#x7406;&#x65B9;&#x6CD5;"></a>4 &#x6837;&#x672C;&#x4E0D;&#x5747;&#x8861;&#x7684;&#x5E38;&#x7528;&#x5904;&#x7406;&#x65B9;&#x6CD5;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-A类欠采样Undersampling"><span class="nav-number">4.1.</span> <span class="nav-text"><a href="#4-1-A&#x7C7B;&#x6B20;&#x91C7;&#x6837;Undersampling" class="headerlink" title="4.1 A&#x7C7B;&#x6B20;&#x91C7;&#x6837;Undersampling"></a>4.1 A&#x7C7B;&#x6B20;&#x91C7;&#x6837;Undersampling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-B类过采样Oversampling"><span class="nav-number">4.2.</span> <span class="nav-text"><a href="#4-2-B&#x7C7B;&#x8FC7;&#x91C7;&#x6837;Oversampling" class="headerlink" title="4.2 B&#x7C7B;&#x8FC7;&#x91C7;&#x6837;Oversampling"></a>4.2 B&#x7C7B;&#x8FC7;&#x91C7;&#x6837;Oversampling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-B类数据合成Synthetic-Data-Generation"><span class="nav-number">4.3.</span> <span class="nav-text"><a href="#4-3-B&#x7C7B;&#x6570;&#x636E;&#x5408;&#x6210;Synthetic-Data-Generation" class="headerlink" title="4.3 B&#x7C7B;&#x6570;&#x636E;&#x5408;&#x6210;Synthetic Data Generation"></a>4.3 B&#x7C7B;&#x6570;&#x636E;&#x5408;&#x6210;Synthetic Data Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-代价敏感学习"><span class="nav-number">4.4.</span> <span class="nav-text"><a href="#4-4-&#x4EE3;&#x4EF7;&#x654F;&#x611F;&#x5B66;&#x4E60;" class="headerlink" title="4.4 &#x4EE3;&#x4EF7;&#x654F;&#x611F;&#x5B66;&#x4E60;"></a>4.4 &#x4EE3;&#x4EF7;&#x654F;&#x611F;&#x5B66;&#x4E60;</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-总结"><span class="nav-number">5.</span> <span class="nav-text"><a href="#5-&#x603B;&#x7ED3;" class="headerlink" title="5. &#x603B;&#x7ED3;"></a>5. &#x603B;&#x7ED3;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-思考题"><span class="nav-number">6.</span> <span class="nav-text"><a href="#6-&#x601D;&#x8003;&#x9898;" class="headerlink" title="6. &#x601D;&#x8003;&#x9898;"></a>6. &#x601D;&#x8003;&#x9898;</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2016 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://Jason.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2017/12/09/决策树与随机森林/';
          this.page.identifier = '2017/12/09/决策树与随机森林/';
          this.page.title = '决策树与随机森林';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://Jason.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  

  

  

</body>
</html>
